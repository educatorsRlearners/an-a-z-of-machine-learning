{
  
    
        "post0": {
            "title": "E is for Exploratory Data Analysis - Categorical Data",
            "content": "What is Exploratory Data Analysis(EDA)? . While I know answered these questions in the last post, I also know that all learning is repetition, so I&#39;ll do it again :grin: . EDA is an ethos for how we scrutinize data including, but not limited to: . what we for | the approach we employ | and the decisions we reach1 | . Why is it done? . Two main reasons: . If we collected the data ourselves, we need to know if our data suits our needs or if we need to collect more/different data. . | If we didn&#39;t collect the data ourselves, we need to come to interrogate the data to answer the &quot;5 W&#39;s&quot; . | What kind of data do we have (i.e. numeric, categorical)? | When was the data collected? There could be more recent data which we could collect which would better inform our model. | How much data do we have? Also, how was the data collected? | Why was the data collected? The original motivation could highlight potential areas of bias in the data. | Who collected the data? | . Some of those questions can&#39;t necessarily be answered by looking at the data alone which is fine because nothing comes from nothing; someone will know the answers all we have to do is know where to look and whom to ask. . How do we do it in Python? . As always, I&#39;ll follow the steps outlined in Hands-on Machine Learning with Scikit-Learn, Keras &amp; TensorFlow . Step 1: Frame the Problem . &quot;Given a set of features, can we determine how old someone needs to be to read a book?&quot; . Step 2: Get the Data . We&#39;ll be using the same dataset as in the previous post. . Step 3: Explore the Data to Gain Insights (i.e. EDA) . As always, import the essential libraries, then load the data. . #For data manipulation import pandas as pd import numpy as np #For visualization import seaborn as sns import matplotlib.pyplot as plt import missingno as msno url = &#39;https://raw.githubusercontent.com/educatorsRlearners/book-maturity/master/csv/book_info_complete.csv&#39; df = pd.read_csv(url) . To review, . How much data do we have? . df.shape . (5816, 24) . 23 features | one target | 5,816 observations | . What type of data do we have? . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5816 entries, 0 to 5815 Data columns (total 24 columns): # Column Non-Null Count Dtype -- -- 0 title 5816 non-null object 1 description 5816 non-null object 2 plot 5816 non-null object 3 csm_review 5816 non-null object 4 need_to_know 5816 non-null object 5 par_rating 2495 non-null float64 6 kids_rating 3026 non-null float64 7 csm_rating 5816 non-null int64 8 Author 5468 non-null object 9 Genre 5816 non-null object 10 Topics 3868 non-null object 11 Book type 5816 non-null object 12 Publisher 5675 non-null object 13 Publication date 5816 non-null object 14 Publisher&#39;s recommended age(s) 4647 non-null object 15 Number of pages 5767 non-null float64 16 Available on 3534 non-null object 17 Last updated 5816 non-null object 18 Illustrator 2490 non-null object 19 Authors 348 non-null object 20 Awards 68 non-null object 21 Publishers 33 non-null object 22 Award 415 non-null object 23 Illustrators 61 non-null object dtypes: float64(3), int64(1), object(20) memory usage: 1.1+ MB . Looks like mostly categorical with some numeric. . Lets take a closer look. . df.head().T . 0 1 2 3 4 . title The Third Twin | Small Damages | The School for Good and Evil, Book 1 | Agent of Chaos: The X-Files Origins, Book 1 | Crossing Ebenezer Creek | . description Gripping thriller skimps on character developm... | Luminous story of pregnant teen&#39;s summer in Sp... | Fractured fairy tale has plenty of twists for ... | Series pictures Mulder as teen, captures essen... | Heartbreaking novel follows freed slaves on Sh... | . plot Twins Ava and Alexa &quot;Lexi&quot; Rios live in an aff... | It&#39;s the summer of 1996, which 18-year-old Ken... | When best friends Sophie and Agatha are stolen... | Set in 1979, AGENT OF CHAOS follows a 17-year-... | CROSSING EBENEZER CREEK is a YA novel from awa... | . csm_review THE THIRD TWIN has an interesting, compelling ... | This could well have been a minefield of clich... | The School for Good and Evil is no run-of-the-... | Popular TV characters don&#39;t always make a smoo... | Beautifully written and poetically rendered, t... | . need_to_know Parents need to know that The Third Twin is a ... | Parents need to know that Small Damages is nar... | Parents need to know that The School for Good ... | Parents need to know that Agent of Chaos: The ... | Parents need to know that Crossing Ebenezer Cr... | . par_rating 17 | NaN | 11 | NaN | NaN | . kids_rating 14 | 14 | 11 | NaN | NaN | . csm_rating 12 | 14 | 8 | 13 | 13 | . Author CJ Omololu | Beth Kephart | Soman Chainani | Kami Garcia | Tonya Bolden | . Genre Mystery | Coming of Age | Fairy Tale | Science Fiction | Historical Fiction | . Topics Adventures, Brothers and Sisters, Friendship, ... | Friendship, History, Horses and Farm Animals | Magic and Fantasy, Princesses, Fairies, Mermai... | Magic and Fantasy, Adventures, Great Boy Role ... | Friendship, History | . Book type Fiction | Fiction | Fiction | Fiction | Fiction | . Publisher Delacorte Press | Philomel | HarperCollins Children&#39;s Books | Imprint | Bloomsbury Children&#39;s Books | . Publication date February 24, 2015 | July 19, 2012 | May 14, 2013 | January 3, 2017 | May 30, 2017 | . Publisher&#39;s recommended age(s) 12 - 18 | 14 - 17 | 8 - 17 | 14 - 18 | NaN | . Number of pages 336 | 304 | 496 | 320 | 240 | . Available on Nook, Hardback, iBooks, Kindle | Nook, Hardback, iBooks, Kindle | Nook, Audiobook (unabridged), Hardback, iBooks... | Nook, Audiobook (abridged), Hardback, iBooks, ... | Nook, Audiobook (unabridged), Hardback, Kindle | . Last updated June 19, 2019 | May 06, 2019 | October 18, 2017 | June 19, 2019 | January 18, 2019 | . Illustrator NaN | NaN | Iacopo Bruno | NaN | NaN | . Authors NaN | NaN | NaN | NaN | NaN | . Awards NaN | NaN | NaN | NaN | NaN | . Publishers NaN | NaN | NaN | NaN | NaN | . Award NaN | NaN | NaN | NaN | NaN | . Illustrators NaN | NaN | NaN | NaN | NaN | . Again, I collected the data so I know the target is csm_rating which is the minimum age Common Sense Media (CSM) says a reader should be for the given book. . Also, we have essentially three types of features: . Numeric par_rating : Ratings of the book by parents | kids_rating : Ratings of the book by children | :dart:csm_rating : Ratings of the books by Common Sense Media | Number of pages : Length of the book | Publisher&#39;s recommended age(s): Self explanatory | . | . Date Publication date : When the book was published | Last updated: When the book&#39;s information was updated on the website | . | . with the rest of the features being text; these features will be our focus for today. . Step 3.1 Housekeeping . To make inspecting a little easier, lets clean those column names. 3 . df.columns . Index([&#39;title&#39;, &#39;description&#39;, &#39;plot&#39;, &#39;csm_review&#39;, &#39;need_to_know&#39;, &#39;par_rating&#39;, &#39;kids_rating&#39;, &#39;csm_rating&#39;, &#39;author&#39;, &#39;genre&#39;, &#39;topics&#39;, &#39;book_type&#39;, &#39;publisher&#39;, &#39;publication_date&#39;, &#39;publisher&#39;s_recommended_ages&#39;, &#39;number_of_pages&#39;, &#39;available_on&#39;, &#39;last_updated&#39;, &#39;illustrator&#39;, &#39;authors&#39;, &#39;awards&#39;, &#39;publishers&#39;, &#39;award&#39;, &#39;illustrators&#39;], dtype=&#39;object&#39;) . df.columns = df.columns.str.strip().str.lower().str.replace(&#39; &#39;, &#39;_&#39;).str.replace(&#39;(&#39;, &#39;&#39;).str.replace(&#39;)&#39;, &#39;&#39;) . df.columns . Index([&#39;title&#39;, &#39;description&#39;, &#39;plot&#39;, &#39;csm_review&#39;, &#39;need_to_know&#39;, &#39;par_rating&#39;, &#39;kids_rating&#39;, &#39;csm_rating&#39;, &#39;author&#39;, &#39;genre&#39;, &#39;topics&#39;, &#39;book_type&#39;, &#39;publisher&#39;, &#39;publication_date&#39;, &#39;publisher&#39;s_recommended_ages&#39;, &#39;number_of_pages&#39;, &#39;available_on&#39;, &#39;last_updated&#39;, &#39;illustrator&#39;, &#39;authors&#39;, &#39;awards&#39;, &#39;publishers&#39;, &#39;award&#39;, &#39;illustrators&#39;], dtype=&#39;object&#39;) . Much better. . Now lets subset the data frame so we only have the features of interest. . Given there are twice as many text features compared to non-text features, and the fact that I&#39;m lazy efficient, I&#39;ll create a list of the features I don&#39;t want . non_text = [&#39;par_rating&#39;, &#39;kids_rating&#39;, &#39;csm_rating&#39;, &#39;number_of_pages&#39;, &quot;publisher&#39;s_recommended_ages&quot;, &quot;publication_date&quot;, &quot;last_updated&quot;] . and use it to keep the features I do want. . df_text = df.drop(df[non_text], axis=1) . Voila! . df_text.head().T . 0 1 2 3 4 . title The Third Twin | Small Damages | The School for Good and Evil, Book 1 | Agent of Chaos: The X-Files Origins, Book 1 | Crossing Ebenezer Creek | . description Gripping thriller skimps on character developm... | Luminous story of pregnant teen&#39;s summer in Sp... | Fractured fairy tale has plenty of twists for ... | Series pictures Mulder as teen, captures essen... | Heartbreaking novel follows freed slaves on Sh... | . plot Twins Ava and Alexa &quot;Lexi&quot; Rios live in an aff... | It&#39;s the summer of 1996, which 18-year-old Ken... | When best friends Sophie and Agatha are stolen... | Set in 1979, AGENT OF CHAOS follows a 17-year-... | CROSSING EBENEZER CREEK is a YA novel from awa... | . csm_review THE THIRD TWIN has an interesting, compelling ... | This could well have been a minefield of clich... | The School for Good and Evil is no run-of-the-... | Popular TV characters don&#39;t always make a smoo... | Beautifully written and poetically rendered, t... | . need_to_know Parents need to know that The Third Twin is a ... | Parents need to know that Small Damages is nar... | Parents need to know that The School for Good ... | Parents need to know that Agent of Chaos: The ... | Parents need to know that Crossing Ebenezer Cr... | . author CJ Omololu | Beth Kephart | Soman Chainani | Kami Garcia | Tonya Bolden | . genre Mystery | Coming of Age | Fairy Tale | Science Fiction | Historical Fiction | . topics Adventures, Brothers and Sisters, Friendship, ... | Friendship, History, Horses and Farm Animals | Magic and Fantasy, Princesses, Fairies, Mermai... | Magic and Fantasy, Adventures, Great Boy Role ... | Friendship, History | . book_type Fiction | Fiction | Fiction | Fiction | Fiction | . publisher Delacorte Press | Philomel | HarperCollins Children&#39;s Books | Imprint | Bloomsbury Children&#39;s Books | . available_on Nook, Hardback, iBooks, Kindle | Nook, Hardback, iBooks, Kindle | Nook, Audiobook (unabridged), Hardback, iBooks... | Nook, Audiobook (abridged), Hardback, iBooks, ... | Nook, Audiobook (unabridged), Hardback, Kindle | . illustrator NaN | NaN | Iacopo Bruno | NaN | NaN | . authors NaN | NaN | NaN | NaN | NaN | . awards NaN | NaN | NaN | NaN | NaN | . publishers NaN | NaN | NaN | NaN | NaN | . award NaN | NaN | NaN | NaN | NaN | . illustrators NaN | NaN | NaN | NaN | NaN | . So many questions come to mind: . How many missing values do we have? | How long are the descriptions? | What&#39;s the difference between csm_review and need_to_know? | Similarly, what the difference between description and plot? | How many different authors do we have in the dataset? | How many types of books do we have? | and I&#39;m sure more will arise once we start. . Where to start? Lets answer the easiest question first :grin: . How many missing values do we have? . A cursory glance at the output above indicates there are potentially a ton of missing values; lets inspect this hunch visually. . msno.bar(df_text, sort=&#39;descending&#39;); . Hunch confirmed: 10 the 17 columns are missing values with some being practically empty. . To get a precise count, we can use sidetable.2 . import sidetable df_text.stb.missing(clip_0=True, style=True) . Missing Total Percent . publishers 5783 | 5,816 | 99.43% | . illustrators 5755 | 5,816 | 98.95% | . awards 5748 | 5,816 | 98.83% | . authors 5468 | 5,816 | 94.02% | . award 5401 | 5,816 | 92.86% | . illustrator 3326 | 5,816 | 57.19% | . available_on 2282 | 5,816 | 39.24% | . topics 1948 | 5,816 | 33.49% | . author 348 | 5,816 | 5.98% | . publisher 141 | 5,816 | 2.42% | . OK, we have lots of missing values and several columns which appear to be measuring similar features (i.e., authors, illustrators, publishers, awards) so lets inspect these features in pairs. . author and authors . Every book has an author, even if the author is &quot;Anonymous,&quot; so then why do we essentially have two columns for the same thing? . :thinking: author is for books with a single writer whereas authors is for books with multiple authors like Good Omens. . Let&#39;s test that theory. . msno.matrix(df_text.loc[:, [&#39;author&#39;, &#39;authors&#39;]]); . Bazinga! . We have a perfect correlation between missing data for author and authors but lets&#39; have a look just in case. . df_text.loc[df_text[&#39;author&#39;].isna() &amp; df_text[&quot;authors&quot;].notna(), [&#39;title&#39;, &#39;author&#39;, &#39;authors&#39;]].head(10) . title author authors . 6 Miss Educated: An Upper Class Novel #2 | NaN | Hobson Brown, Caroline Says, Taylor Materne | . 27 Zeroes, Book 1 | NaN | Scott Westerfeld, Margo Lanagan, Deborah Bianc... | . 34 Middle School: From Hero to Zero: Middle Schoo... | NaN | James Patterson, Chris Tebbetts | . 47 Legend: The Graphic Novel | NaN | Marie Lu, Leigh Dragoon | . 108 Survivors: Stranded, Book 3 | NaN | Jeff Probst, Chris Tebbetts | . 112 Avatar: The Last Airbender: The Rise of Kyoshi | NaN | F.C. Yee, Michael Dante DiMartino | . 116 I&#39;m Not Dying With You Tonight | NaN | Gilly Segal, Kimberly Jones | . 129 Upside-Down Magic | NaN | Sarah Mlynowski, Lauren Myracle, Emily Jenkins | . 138 The Field Guide: The Spiderwick Chronicles, Bo... | NaN | Holly Black, Tony DiTerlizzi | . 164 We Rise, We Resist, We Raise Our Voices | NaN | Wade Hudson, Cheryl Willis Hudson | . df_text.loc[df_text[&#39;author&#39;].isna() &amp; df_text[&quot;authors&quot;].notna(), [&#39;title&#39;, &#39;author&#39;, &#39;authors&#39;]].tail(10) . title author authors . 5580 Jazzy&#39;s Quest: Adopted and Amazing! | NaN | Carrie Goldman, Juliet Bond | . 5635 A Short Tale About a Long Dog: Here&#39;s Hank, Bo... | NaN | Henry Winkler, Lin Oliver | . 5663 Middle School, the Worst Years of My Life | NaN | James Patterson, Chris Tebbetts | . 5668 Curious George Goes to the Hospital | NaN | Margret Rey, H. A. Rey | . 5701 Nexus: Zeroes, Book 3 | NaN | Scott Westerfeld, Margo Lanagan, Deborah Bianc... | . 5710 Poco Loco | NaN | J.R. Krause, Maria Chua | . 5711 Jacky Ha-Ha: My Life Is a Joke | NaN | James Patterson, Chris Grabenstein | . 5723 Hidden Figures: The True Story of Four Black W... | NaN | Margot Lee Shetterly, Winifred Conkling | . 5756 The Princess in Black and the Science Fair Scare | NaN | Shannon Hale, Dean Hale | . 5812 Secrets of the Terra-Cotta Soldier | NaN | Ying Chang Compestine, Vinson Compestine | . My curiosity is satiated. . Now the question is how to successfully merge the two columns? . We could replace the NaN in author with the: . values in authors | word multiple | first author in authors | more/most popular of the authors in authors | . and I&#39;m sure I could come up with even more if I thought about/Googled it but the key is to understand that no matter what we choose, it will have consequences when we build our model3. . Next question which comes to mind is how many different authors are there? . df_text.loc[:, &#39;author&#39;].nunique() . 2668 . Wow! Nearly half our our observations contain a unique name meaning this feature has high cardinality. . :thinking: Which authors are most represented in the data set? . Lets create a frequency table to find out. . author_counts = df_text.loc[:, [&quot;title&quot;, &#39;author&#39;]].groupby(&#39;author&#39;).count().reset_index() author_counts.sort_values(&#39;title&#39;, ascending=False).head(10) . author title . 670 Dr. Seuss | 39 | . 2167 Rick Riordan | 26 | . 1470 Kevin Henkes | 25 | . 1916 Mo Willems | 24 | . 1965 Neil Gaiman | 18 | . 767 Eoin Colfer | 17 | . 2619 Walter Dean Myers | 16 | . 2643 William Joyce | 16 | . 1103 Jeff Kinney | 15 | . 1579 Lemony Snicket | 15 | . Given that I&#39;ve scraped the data from a website focusing on children, teens, and young adults, the results above only make sense; authors like Dr. Seuss, Eoin Coifer, and Lemony Snicket are famous children&#39;s authors whereas Rick Riordan, Walter Dean Myers occupy the teen/young adult space and Neil Gaiman writes across ages. . :thnking: How many authors are only represented once? . That&#39;s easy to check. . ax = author_counts[&#39;title&#39;].value_counts().plot.barh() ax.invert_yaxis(); . Wow! So the overwhelming majority of the authors have one title in our data set. . Just for fun, lets get an actual percentage. . round(sum(author_counts[&#39;title&#39;]==1)/len(author_counts), 2) . 0.62 . Just under two-thirds. . Why does that matter? . When it comes time to build our model we&#39;ll need to either label encode, one-hot encode, or hash this feature and whichever we decide to do will end up effecting the model profoundly due to the high cardinality of this feature; however, we&#39;ll deal with all this another time :grin:. . illustrator and illustrators . Missing values can be quite informative. . :thinking: What types of books typically have illustrators? :bulb: Children&#39;s books! . Therefore, if a book&#39;s entries for both illustrator and illustrators is blank, that probably means that book doesn&#39;t have illustrations which would mean it is more likely to be for older children. . Let&#39;s test this theory in the simplest way I can think of :smile: . #Has an illustrator df.loc[df[&#39;illustrator&#39;].notna() | df[&#39;illustrators&#39;].notna(), [&#39;csm_rating&#39;]].hist(); . #Doesn&#39;t have an illustrator df.loc[df[&#39;illustrators&#39;].isna() &amp; df[&quot;illustrator&quot;].isna(), [&#39;csm_rating&#39;]].hist(); . :bulb: Who the illustrator is doesn&#39;t matter as much as whether there is an illustrator. . Looks like when I do some feature engineering I&#39;ll need to create a has_illustrator feature. . Summary . :ballot_box_with_check: numeric data | :ballot_box_with_check: categorical data | :black_square_button: images (book covers) | . Two down; one to go! . Going forward, my key points to remember are: . Does the shape of the data make sense? . Based on my problem statement, I do not need normally distributed data. However, based on the question I&#39;m trying to solve, I might expect the data to fit a certain distribution. . Similarly, are the values what I expect? . What would have happened if the only ratings I had were for 4 year olds? Clearly, I would have made a mistake somewhere along the line and would have to go back and fix it. . Also, I have to ask if the data makes sense or if I have outliers. . What&#39;s missing? . There will always be missing values. How many and in which features is going to drive a lot of feature engineering questions. . Speaking of which... . Are all the features I want present? . The numeric features I have are pretty complete, but what would happen if I combined the par_rating with the kids_rating to create a new feature? Would the two features combined be more valuable than either one on its own? Only one way to find out :smile: . Happy coding! . Footnotes . 1. Adapted from Engineering Statistics Handbook↩ . 2. Be sure to check out this excellent post by Jeff Hale for more examples on how to use this package↩ . 3. See this post on Smarter Ways to Encode Categorical Data↩ . 4. Big Thank You to Chaim Gluck for providing this tip↩ .",
            "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/draft/2020/06/22/e-is-for-eda.html",
            "relUrl": "/draft/2020/06/22/e-is-for-eda.html",
            "date": " • Jun 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "E is for Exploratory Data Analysis - Numeric Data",
            "content": "What is Exploratory Data Analysis(EDA)? . EDA is the process of getting to know our data primarily through simple visualizations before fitting a model. As Wickham and Grolemund state, EDA is more an attitude than a scripted list of steps which must be carried out1. . Why is it done? . Two main reasons: . If we collected the data ourselves to solve a problem, we need to determine whether our data is sufficient for solving that problem. . | If we didn&#39;t collect the data ourselves, we need to have a basic understanding of the type, quantity, quality, and possible relationships between the features in our data. . | How do we do it in Python? . While I could use a toy data set, like in my last post, after seeing seeing tweets like this: 1. Isn’t iris the iris of ML2. Isn’t gapminder the iris of EDA . &mdash; David Robinson (@drob) July 23, 2018 and listening to Hugo Bowne-Anderson on DataFramed bemoan the over use of the Iris and Titanic datasets, I&#39;m feeling inspired to use my own data :grin: . As always, I&#39;ll follow the steps outlined in Hands-on Machine Learning with Scikit-Learn, Keras &amp; TensorFlow . Step 1: Frame the Problem . &quot;Given a set of features, can we determine how old someone needs to be to read a book?&quot; . Step 2: Get the Data . To answer the question above, I sourced labeled data by scraping Common Sense Media&#39;s Book Reviews using BeautifulSoup and then wrote the data to a csv.2 . . Now that we have our data lets move on to... . Step 3: Explore the Data to Gain Insights (i.e. EDA) . As always, import the essential libraries, then load the data. . #For data manipulation import pandas as pd import numpy as np #For visualization import seaborn as sns import matplotlib.pyplot as plt import missingno as msno url = &#39;https://raw.githubusercontent.com/educatorsRlearners/book-maturity/master/csv/book_info_complete.csv&#39; df = pd.read_csv(url) . Time to start asking and answering some basic questions: . How much data do we have? | . df.shape . (5816, 24) . OK, so we have 23 features and one target as well as 5,816 observations. . Why do we have fewer than in the screenshot above? . Because Common Sense Media is constantly adding new reviews to their website, meaning they&#39;ve added nearly 100 books to their site since I completed my project at the end of March 2020. . What type of data do we have? | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5816 entries, 0 to 5815 Data columns (total 24 columns): # Column Non-Null Count Dtype -- -- 0 title 5816 non-null object 1 description 5816 non-null object 2 plot 5816 non-null object 3 csm_review 5816 non-null object 4 need_to_know 5816 non-null object 5 par_rating 2495 non-null float64 6 kids_rating 3026 non-null float64 7 csm_rating 5816 non-null int64 8 Author 5468 non-null object 9 Genre 5816 non-null object 10 Topics 3868 non-null object 11 Book type 5816 non-null object 12 Publisher 5675 non-null object 13 Publication date 5816 non-null object 14 Publisher&#39;s recommended age(s) 4647 non-null object 15 Number of pages 5767 non-null float64 16 Available on 3534 non-null object 17 Last updated 5816 non-null object 18 Illustrator 2490 non-null object 19 Authors 348 non-null object 20 Awards 68 non-null object 21 Publishers 33 non-null object 22 Award 415 non-null object 23 Illustrators 61 non-null object dtypes: float64(3), int64(1), object(20) memory usage: 1.1+ MB . Looks like a mix of strings and floats. . Lets take a closer look. . df.head().T . 0 1 2 3 4 . title The Third Twin | Small Damages | The School for Good and Evil, Book 1 | Agent of Chaos: The X-Files Origins, Book 1 | Crossing Ebenezer Creek | . description Gripping thriller skimps on character developm... | Luminous story of pregnant teen&#39;s summer in Sp... | Fractured fairy tale has plenty of twists for ... | Series pictures Mulder as teen, captures essen... | Heartbreaking novel follows freed slaves on Sh... | . plot Twins Ava and Alexa &quot;Lexi&quot; Rios live in an aff... | It&#39;s the summer of 1996, which 18-year-old Ken... | When best friends Sophie and Agatha are stolen... | Set in 1979, AGENT OF CHAOS follows a 17-year-... | CROSSING EBENEZER CREEK is a YA novel from awa... | . csm_review THE THIRD TWIN has an interesting, compelling ... | This could well have been a minefield of clich... | The School for Good and Evil is no run-of-the-... | Popular TV characters don&#39;t always make a smoo... | Beautifully written and poetically rendered, t... | . need_to_know Parents need to know that The Third Twin is a ... | Parents need to know that Small Damages is nar... | Parents need to know that The School for Good ... | Parents need to know that Agent of Chaos: The ... | Parents need to know that Crossing Ebenezer Cr... | . par_rating 17 | NaN | 11 | NaN | NaN | . kids_rating 14 | 14 | 11 | NaN | NaN | . csm_rating 12 | 14 | 8 | 13 | 13 | . Author CJ Omololu | Beth Kephart | Soman Chainani | Kami Garcia | Tonya Bolden | . Genre Mystery | Coming of Age | Fairy Tale | Science Fiction | Historical Fiction | . Topics Adventures, Brothers and Sisters, Friendship, ... | Friendship, History, Horses and Farm Animals | Magic and Fantasy, Princesses, Fairies, Mermai... | Magic and Fantasy, Adventures, Great Boy Role ... | Friendship, History | . Book type Fiction | Fiction | Fiction | Fiction | Fiction | . Publisher Delacorte Press | Philomel | HarperCollins Children&#39;s Books | Imprint | Bloomsbury Children&#39;s Books | . Publication date February 24, 2015 | July 19, 2012 | May 14, 2013 | January 3, 2017 | May 30, 2017 | . Publisher&#39;s recommended age(s) 12 - 18 | 14 - 17 | 8 - 17 | 14 - 18 | NaN | . Number of pages 336 | 304 | 496 | 320 | 240 | . Available on Nook, Hardback, iBooks, Kindle | Nook, Hardback, iBooks, Kindle | Nook, Audiobook (unabridged), Hardback, iBooks... | Nook, Audiobook (abridged), Hardback, iBooks, ... | Nook, Audiobook (unabridged), Hardback, Kindle | . Last updated June 19, 2019 | May 06, 2019 | October 18, 2017 | June 19, 2019 | January 18, 2019 | . Illustrator NaN | NaN | Iacopo Bruno | NaN | NaN | . Authors NaN | NaN | NaN | NaN | NaN | . Awards NaN | NaN | NaN | NaN | NaN | . Publishers NaN | NaN | NaN | NaN | NaN | . Award NaN | NaN | NaN | NaN | NaN | . Illustrators NaN | NaN | NaN | NaN | NaN | . The picture is coming into focus. Again, since I collected the data, I know that the target is csm_rating which is the minimum age Common Sense Media (CSM) says a reader should be for the given book. . Also, we have essentially three types of features: . Numeric par_rating : Ratings of the book by parents | kids_rating : Ratings of the book by children | :dart:csm_rating : Ratings of the books by Common Sense Media | Number of pages : Length of the book | Publisher&#39;s recommended age(s): Self explanatory | . | . Date Publication date : When the book was published | Last updated: When the book&#39;s information was updated | . | . with all other features being text. . To make inspecting a little easier, lets clean those column names. 3 . df.columns . Index([&#39;title&#39;, &#39;description&#39;, &#39;plot&#39;, &#39;csm_review&#39;, &#39;need_to_know&#39;, &#39;par_rating&#39;, &#39;kids_rating&#39;, &#39;csm_rating&#39;, &#39;Author&#39;, &#39;Genre&#39;, &#39;Topics&#39;, &#39;Book type&#39;, &#39;Publisher&#39;, &#39;Publication date&#39;, &#39;Publisher&#39;s recommended age(s)&#39;, &#39;Number of pages&#39;, &#39;Available on&#39;, &#39;Last updated&#39;, &#39;Illustrator&#39;, &#39;Authors&#39;, &#39;Awards&#39;, &#39;Publishers&#39;, &#39;Award&#39;, &#39;Illustrators&#39;], dtype=&#39;object&#39;) . df.columns = df.columns.str.strip().str.lower().str.replace(&#39; &#39;, &#39;_&#39;).str.replace(&#39;(&#39;, &#39;&#39;).str.replace(&#39;)&#39;, &#39;&#39;) . df.columns . Index([&#39;title&#39;, &#39;description&#39;, &#39;plot&#39;, &#39;csm_review&#39;, &#39;need_to_know&#39;, &#39;par_rating&#39;, &#39;kids_rating&#39;, &#39;csm_rating&#39;, &#39;author&#39;, &#39;genre&#39;, &#39;topics&#39;, &#39;book_type&#39;, &#39;publisher&#39;, &#39;publication_date&#39;, &#39;publisher&#39;s_recommended_ages&#39;, &#39;number_of_pages&#39;, &#39;available_on&#39;, &#39;last_updated&#39;, &#39;illustrator&#39;, &#39;authors&#39;, &#39;awards&#39;, &#39;publishers&#39;, &#39;award&#39;, &#39;illustrators&#39;], dtype=&#39;object&#39;) . Much better. . Given the number and variety of features, I&#39;ll focus on the numeric features in this post and analyze the text features in a part II. . Therefore, lets subset the data frame work with only the features of interest. . numeric = [&#39;par_rating&#39;, &#39;kids_rating&#39;, &#39;csm_rating&#39;, &#39;number_of_pages&#39;, &quot;publisher&#39;s_recommended_ages&quot;] . df_numeric = df[numeric] . df_numeric.head() . par_rating kids_rating csm_rating number_of_pages publisher&#39;s_recommended_ages . 0 17.0 | 14.0 | 12 | 336.0 | 12 - 18 | . 1 NaN | 14.0 | 14 | 304.0 | 14 - 17 | . 2 11.0 | 11.0 | 8 | 496.0 | 8 - 17 | . 3 NaN | NaN | 13 | 320.0 | 14 - 18 | . 4 NaN | NaN | 13 | 240.0 | NaN | . :thumbsdown: publisher&#39;s_recommended_ages is a range instead of a single value. :thumbsdown: It&#39;s the wrong data type (i.e., non-numeric) . df_numeric.dtypes . par_rating float64 kids_rating float64 csm_rating int64 number_of_pages float64 publisher&#39;s_recommended_ages object dtype: object . :thumbsup: We can fix both of those issues. . Given that we only care about the minimum age, we can: . split the string on the hyphen | keep only the first value since that will be the minimum age recommended by the publisher | convert the column to numeric | . #Create a column with the minimum age df_numeric[&#39;pub_rating&#39;] = df.loc[:, &quot;publisher &#39;s_recommended_ages&quot;].str.split(&quot;-&quot;, n=1, expand=True)[0] #Set the column as numeric df_numeric.loc[:, &#39;pub_rating&#39;] = pd.to_numeric(df_numeric[&#39;pub_rating&#39;]) . df_numeric.head().T . 0 1 2 3 4 . par_rating 17 | NaN | 11 | NaN | NaN | . kids_rating 14 | 14 | 11 | NaN | NaN | . csm_rating 12 | 14 | 8 | 13 | 13 | . number_of_pages 336 | 304 | 496 | 320 | 240 | . publisher&#39;s_recommended_ages 12 - 18 | 14 - 17 | 8 - 17 | 14 - 18 | NaN | . pub_rating 12 | 14 | 8 | 14 | NaN | . Now we can drop the unnecessary column. . df_numeric = df_numeric.drop(columns=&quot;publisher&#39;s_recommended_ages&quot;) df_numeric.head().T . 0 1 2 3 4 . par_rating 17.0 | NaN | 11.0 | NaN | NaN | . kids_rating 14.0 | 14.0 | 11.0 | NaN | NaN | . csm_rating 12.0 | 14.0 | 8.0 | 13.0 | 13.0 | . number_of_pages 336.0 | 304.0 | 496.0 | 320.0 | 240.0 | . pub_rating 12.0 | 14.0 | 8.0 | 14.0 | NaN | . Everything is in order so lets dig in and start by . Inspecting the target . df_numeric.loc[:, &#39;csm_rating&#39;].describe() . count 5816.000000 mean 9.161623 std 3.881257 min 2.000000 25% 6.000000 50% 9.000000 75% 13.000000 max 17.000000 Name: csm_rating, dtype: float64 . Good news! We do not have any missing values for our target! Also, we can see the lowest recommended age for a book is 2 years old, which has to be a picture book, while the highest is 17. . All useful info, but what does our target look like? . df_numeric.loc[:, &#39;csm_rating&#39;].plot(kind= &quot;hist&quot;, bins=range(2,18), figsize=(24,10), xticks=range(2,18), fontsize=16); . Hmmm. Two thoughts: . First, the distribution is multimodal so when we split the data intotrain-test-validate splits, we&#39;ll need to do a stratified random sample. Also, the book recommendations seem to fall into one of three categories: really young readers, (e.g., 5 years old), tweens, and teens or older. . :bulb: Given this distribution, we could simplify our task from predicting an exact age and instead predict an age group. Something to keep in mind for future research. . Moving on. . Missing Values . Looking back at the output from df.info(), its obvious that several features have missing values but let&#39;s visualize it to make it clear. . msno.bar(df_numeric); . Good News! . There are fewer than 50 missing values for number_of_pages | . Bad News! . pub_rating is missing a thousand values | Nearly half of the kids_rating are missing | More than half of the par_rating are missing. | . When we get to the cleaning/feature engineering stage, we&#39;ll have to decide whether it&#39;s better to drop or impute) the missing values. However, before we do that, lets see visualize the data to get a better feel for it. . df_numeric[&#39;kids_rating&#39;].plot(kind= &quot;hist&quot;, bins=range(2,18), figsize=(24,10), xticks=range(2,18), fontsize=16); . Hmmm. Looks like the children who wrote the bulk of the reviews think the books they reviewed were suitable for children between the ages of 8 and 14. . What about the parent&#39;s ratings? . df_numeric[&#39;par_rating&#39;].plot(kind= &quot;hist&quot;, bins=range(2,18), figsize=(24,10), xticks=range(2,18), fontsize=16); . Same shape as the kids but a little less pronounced? . Finally, let&#39;s find out what the publishers think. . df_numeric[&#39;pub_rating&#39;].plot(kind= &quot;hist&quot;, bins=range(2,18), figsize=(24,10), xticks=range(2,18), fontsize=16); . This looks promising. Let&#39;s compare it to our target csm_rating. . df_numeric.loc[:, [&#39;pub_rating&#39;,&#39;csm_rating&#39;]].hist(bins=range(2,18), figsize=(20,10)); . :thinking: While not identical by any means, both distributions have the same multimodal shape. . Lets see how well our features correlate. . #Create the correlation matrix corr = df_numeric.corr() #Generate a mask to over the upper-right side of the matrix mask = np.zeros_like(corr) mask[np.triu_indices_from(mask)] = True #Plot the heatmap with correlations with sns.axes_style(&quot;white&quot;): f, ax = plt.subplots(figsize=(8, 6)) ax = sns.heatmap(corr, mask=mask, annot=True, square=True) . :thinking: The pub_rating is nearly a proxy for the target. . :thinking: The parents and kids ratings correlate more strongly with the target than they do with each other. . Looks like we&#39;re going to have to impute those missing values after all. . What about potential outliers? . A final step in EDA is to search for potential outliers which are values significantly different from the norm. . Our dataset could contain outliers for a couple of reasons: . data is miskeyed in, (e.g., someone types &quot;100&quot; instead of &quot;10&quot;) | the observation genuinely is significantly different from the norm 4 | . The good news is that I chose to scrape Common Sense Media&#39;s book reviews because I felt confident in their ratings based on my knowledge of the domain and, given the professionalism of the organization, we can be fairly certain of the veracity of the data. . However, the old maxim of &quot;Trust, but verify&quot; exists for a reason. . This post is already much longer than I had planned so I won&#39;t go through all the numeric features, nor the multiple ways of identifying outliers, but a really simple one to plot the relationships between features so lets investigate the relationship between ratings and book length. . df_numeric.plot.scatter(x=&#39;number_of_pages&#39;, y=&quot;csm_rating&quot;, figsize=(24,10), fontsize=16); . We have spotted our first probable outliers: it is inconceivable for a ~400 page book to be meant for a two or three year old, right? . df.query(&#39;csm_rating &lt; 6 &amp; number_of_pages &gt; 300&#39;)[[&#39;title&#39;,&#39;description&#39;]] . title description . 2516 Beatrix Potter: The Complete Tales | All of Beatrix Potter in one book, pre-K and K. | . 3158 Splat the Cat | Even cats worry about the first day of school. | . 4076 Eloise: The Ultimate Edition | All four stories about the irrepressible Eloise. | . 4463 A Giant Crush | Fun Valentine story stresses self-esteem, frie... | . 4499 The Complete Tales &amp; Poems of Winnie-the-Pooh | Beloved, classic stories and poems in one volume. | . 4954 Mad About Madeline | The stories vary in quality but still delight. | . Well so much for that idea :grin: . To quote the AI Guru, instead of simply relying on printouts and plots, you &quot;should always look at your bleeping data.&quot; . Summary . This post turned out to be part one of what will likely be at least three posts: EDA for: . :ballot_box_with_check: numeric data | :black_square_button: categorical data | :black_square_button: images (book covers) | . Going forward, my key points to remember are: . Does the shape of the data make sense? . Based on my problem statement, I do not need normally distributed data. However, based on the question I&#39;m trying to solve, I might expect the data to fit a certain distribution. . Similarly, are the values what I expect? . What would have happened if the only ratings I had were for 4 year olds? Clearly, I would have made a mistake somewhere along the line and would have to go back and fix it. . Also, I have to ask if the data makes sense or if I have outliers. . What&#39;s missing? . There will always be missing values. How many and in which features is going to drive a lot of feature engineering questions. . Speaking of which... . Are all the features I want present? . The numeric features I have are pretty complete, but what would happen if I combined the par_rating with the kids_rating to create a new feature? Would the two features combined be more valuable than either one on its own? Only one way to find out :smile: . Happy coding! . Footnotes . 1. Chapter 7: Exploratory Data Analysis in R for Data Science by Hadley Wickham &amp; Garrett Grolemund↩ . 2. You can find the code here↩ . 3. Big Thank You to Chaim Gluck for providing this tip↩ . 4. Yao Ming really was 2 meters tall when he was 13↩ .",
            "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/e/2020/06/15/e-is-for-eda.html",
            "relUrl": "/e/2020/06/15/e-is-for-eda.html",
            "date": " • Jun 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "D is for Decision Tree",
            "content": "What is a Decision Tree? . Decision Trees are an algorithm used for either classification or regression tasks. . How do they work? . While the math is fairly complicated, the concept is incredibly straightforward: . start with all observations in one group | identify a binary question, (i.e., yes/no, over/under) resulting in two groups which are as distinct from eachother as possible | repeat step two until every subgroup is homogeneous or some other metric has been achieved | What are the advantages? . Transparency: They are considered a white/glass box algorithm because you can see what decisions the algorithm made which leads to, . Interpretability: Again, since you can see which decisions were made, it&#39;s easy to comprehend and explain the predictions. . Ease: Data trees DO NOT require feature scaling or normalization1. . What are the disadvantages? . Overfitting: Since they try to find the purest groups, they have a tendency to overfit. . Non-Linear Data: Relationships in the data between features are not considered. . How do we train a decision tree? . Step 0: Frame the Problem . I&#39;m a task based person so let&#39;s set a problem. . Can we determine who would have survived the Titanic? . Step 1: Collect/Load our Data . Now let&#39;s get the data. . import seaborn as sns sns.set(palette=&quot;colorblind&quot;) titanic = sns.load_dataset(&quot;titanic&quot;) . Step 2: Inspect our Data . Do we have any missing values? . titanic.isnull().sum() . survived 0 pclass 0 sex 0 age 177 sibsp 0 parch 0 fare 0 embarked 2 class 0 who 0 adult_male 0 deck 688 embark_town 2 alive 0 alone 0 dtype: int64 . Yes we do. . What are the data types? . titanic.dtypes . survived int64 pclass int64 sex object age float64 sibsp int64 parch int64 fare float64 embarked object class category who object adult_male bool deck category embark_town object alive object alone bool dtype: object . We have a mix. . Let&#39;s get a feel for the values by looking at the first five rows. . titanic.head().T . 0 1 2 3 4 . survived 0 | 1 | 1 | 1 | 0 | . pclass 3 | 1 | 3 | 1 | 3 | . sex male | female | female | female | male | . age 22 | 38 | 26 | 35 | 35 | . sibsp 1 | 1 | 0 | 1 | 0 | . parch 0 | 0 | 0 | 0 | 0 | . fare 7.25 | 71.2833 | 7.925 | 53.1 | 8.05 | . embarked S | C | S | S | S | . class Third | First | Third | First | Third | . who man | woman | woman | woman | man | . adult_male True | False | False | False | True | . deck NaN | C | NaN | C | NaN | . embark_town Southampton | Cherbourg | Southampton | Southampton | Southampton | . alive no | yes | yes | yes | no | . alone False | False | True | False | True | . Hmmm. Looks like seaborn has already done some feature engineering (i.e, alone is a combination of sibsp and parch). Much appreciated :smiley: . Now, since this is a toy dataset, I&#39;m not going to do much EDA but, if you want to learn more, you can find some good examples of it here and here. . However, here is an obligatory bar chart of who survived. . sns.catplot(y=&quot;survived&quot;, hue=&quot;sex&quot;, kind=&quot;count&quot;, data=titanic); . Step 3: Prepare the data . In the interest of simplicity, I&#39;m going to make a REALLY basic model by: . dropping all features containing missing values | . titanic = titanic.dropna(axis=1) titanic.info(); . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 survived 891 non-null int64 1 pclass 891 non-null int64 2 sex 891 non-null object 3 sibsp 891 non-null int64 4 parch 891 non-null int64 5 fare 891 non-null float64 6 class 891 non-null category 7 who 891 non-null object 8 adult_male 891 non-null bool 9 alive 891 non-null object 10 alone 891 non-null bool dtypes: bool(2), category(1), float64(1), int64(4), object(3) memory usage: 58.5+ KB . keeping only numerical features | . titanic = titanic.select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]) titanic.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 survived 891 non-null int64 1 pclass 891 non-null int64 2 sibsp 891 non-null int64 3 parch 891 non-null int64 4 fare 891 non-null float64 dtypes: float64(1), int64(4) memory usage: 34.9 KB . Cool. . Time to make the train test split. . from sklearn.model_selection import train_test_split x = titanic.iloc[:, 1:] y = titanic.survived x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42) . and then . Step 4: Fit the model . from sklearn.tree import DecisionTreeClassifier tree_clf = DecisionTreeClassifier(random_state=0) tree_clf = tree_clf.fit(x_train, y_train) . Step 5: Evaluate the model . Visualzing the model . Earlier, I said it was easy to explain the decision the model made. . &quot;How?&quot; you ask? By visualizing it: . from sklearn.tree import export_graphviz from IPython.display import Image import pydotplus dot_data = export_graphviz(tree_clf, out_file=None, feature_names=x_train.columns, class_names=[&#39;perished&#39;, &#39;survived&#39;], rounded=True, filled=True) # Draw graph graph = pydotplus.graph_from_dot_data(dot_data) # Show graph Image(graph.create_png()) . That is a gigantic tree!!! In reality, we&#39;d probably prune it by limiting the depth of the tree in an attempt to avoid overfitting. . But, let see how it performs. . #Predict the response for test dataset y_pred = tree_clf.predict(x_test) from sklearn.metrics import accuracy_score # Model Accuracy, how often is the classifier correct? print(&quot;The training accuracy of our model is %.2f.&quot; % (accuracy_score(y_train, tree_clf.predict(x_train))*100)) print(&quot;The test accuracy of our model is %.2f.&quot; % (accuracy_score(y_test, y_pred) *100)) . The training accuracy of our model is 84.11. The test accuracy of our model is 69.40. . Yep, definitely overfitting. . So, let&#39;s prune our tree by limiting the max_depth to three: . tree_clf = DecisionTreeClassifier(max_depth=3, random_state=0) . retrain our model, . tree_clf = tree_clf.fit(x_train, y_train) . and visualize it one more time. . dot_data = export_graphviz(tree_clf, out_file=None, feature_names=x_train.columns, class_names=[&quot;perished&quot;, &quot;survived&quot;], rounded=True, filled=True) # Draw graph graph = pydotplus.graph_from_dot_data(dot_data) # Show graph Image(graph.create_png()) . That is a much easier tree to comprehend but does it perform better? . #Predict the response for test dataset y_pred = tree_clf.predict(x_test) # Model Accuracy, how often is the classifier correct? print(&quot;The training accuracy of our model is %.2f.&quot; % (accuracy_score(y_train, tree_clf.predict(x_train))*100)) print(&quot;The test accuracy of our model is %.2f.&quot; % (accuracy_score(y_test, y_pred) *100)) . The training accuracy of our model is 73.68. The test accuracy of our model is 71.64. . Indeed it does! . Now imagine how much better it would perform if we added gender and age to the model. . Summary . Decision trees are incredibly powerful because of their: . ease of use | flexibility | simplicity to understand | . However, overfitting can be a real issue so, as with all algorithms, check the training accuracy and tune the hyperparameters as needed. . Also, some of the splits defy common sense; for example, how can anyone have half a sibling or half a parent?2 . Therefore, while the trees maybe easy to interpret, they might not always be logical so, as always, we have to be prepared to defend what we&#39;ve made. . Happy coding! . Further Reading . Hands-On Machine Learning with Scikit-Learn and TensorFlow: &#39;Chapter 6 Decision Trees&#39; DataCamp Decision Tree Classifier Tutorial Understanding the Mathematics Behind Decision Trees Visualizing Decision Trees . Footnotes . 1. Geron, 2019, Page 177↩ . 2. If anyone knows how to set the tree to only split on whole values, please write it in the comments below.↩ .",
            "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/d/2020/05/26/d-is-for-decision-tree.html",
            "relUrl": "/d/2020/05/26/d-is-for-decision-tree.html",
            "date": " • May 26, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "C is for Classification",
            "content": "What is classification? . Classification is one of two types of supervised machine learning tasks with the other being regression. . Key point to remember: supervised learning tasks use features to predict targets, or, in non-tech speak, they use attributes to predict something. For instance, we can take a basketball players height, weight, age, foot-speed and/or multiple other aspects their them to predict how many points they&#39;ll score or whether they will be an all-star. . So what&#39;s the difference between the two? . Regression tasks predict a continuous value For example, how many points someone will score. | . | Classification tasks predict a non-continuous value. For example, whether a player will be an all-star. | . | . How do I know which technique to use? . Answer the following question: &quot;Does my target variable have an order to it?&quot; . For example, my project predicting the recommended age of a reader was a regression task because I was predicting a precise age (i.e., 4 years old). If I was attempting to identify whether a book was suitable for teens or not, then it would have been a classification task since the answer would have been either yes or no. . OK, so classification is only for yes/no, true/false, cat/dog problems, right? . Nope, those are just the easy examples :smiley: . Example 1: Sorting People into Groups . Imagine a scenario where you get a new batch of students every year and have to sort them into houses based on their personality characteristics. . . In this situation, the houses do not have any type of sequence/ranking to them. Sure, Harry definitely didn&#39;t want to be housed in Slytherin, and the sorting hat clearly took that into consideration, but that doesn&#39;t mean Slytherin is closer to Gryffindor like 25 is closer to 30 than it is to 19. . Example 2: Applying Labels . Similarly, if we had a data set containing the ingredients of dishes and attempted to predict the country of origin, we&#39;d be solving a classification problem. Why? Because country names have no numerical order. We can say that Russia is the largest country on Earth or that China has the most people but those are attributes of the country (i.e., land size and population) which are not intrinsic to the name of the country. . Got it! Numbers are regression, words are classification . Sorry but no. . Think back to the book recommendation project I mentioned earlier. Answer this: if I wanted to predict whether a book was for: . small children (2-5 years old) | primary children (6 - 10) | tweens (11 -12) | young adults (13 - 17) | adults (18 +) | . what would you use: regression or classification? . Well, given that the labels have a clear order, you&#39;d definitely want to treat this as a regression problem by coding &#39;small children&#39; as &#39;1&#39; and &#39;adults&#39; as &#39;5&#39;1. . In Sum . Always start by asking &#39;What am I attempting to predict?&#39; Once that question is solved the rest becomes far simpler because as Jung said, &quot;To ask the right question, is already half the problem solved.&quot; . Further Reading . Difference Between Classification and Regression in Machine Learning Regression Versus Classification Machine Learning: What’s the Difference? . Footnotes . 1. In R this type of variable is called an ordered factor↩ .",
            "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/c/2020/05/24/c-is-for-classification.html",
            "relUrl": "/c/2020/05/24/c-is-for-classification.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "B is for Books",
            "content": "While this is not an exhaustive list of the books I&#39;ve read in my journey to become a data scientist, it is comprised of the books I consider my essential reading. . Statistics . The Cartoon Guide to Statistics . An engaging reference book which explains key concepts like descriptives statistics, distributions, probability, hypothesis testing. . The Manga Guide to Statistics . Similar to the title above, but this book has a plot that runs through it. Consequently, if you find analogies easier to remember than formulas, this is a great book for you. . Data Science . R . R for Data Science . After completing several DataCamp courses, I started using R to analyze test-taker data and this book was my go to reference for the tidyverse. . Geocomputation in R . My last position required my team and I to travel throughout East Asia to deliver workshops, presentations, and provide client support. This book taught me how to make maps to visualize our reach and impact. . Blogdown: Creating Websites with R Markdown . I would not have been able to create my previous blog without this book. Simply indispensable if you want to create a blog with RStudio. . Python . A Whirlwind Tour of Python . Need to get the gist of Python in a hurry? This is your book. . Hands-On Machine Learning with Scikit-Learn and TensorFlow . &quot;Chapter 2 End-to-End Machine Learning Project&quot; along with &quot;Appendix B: Machine Learning Project Checklist&quot; are worth the price of the book alone. . Python for Data Analysis . The real name of this book should be Pandas from A-Z. Seriously, work through the examples in Chapter 14 and you will feel infinitely more confident to use pandas in the future. . Concepts . Storytelling with Data . If &quot;a picture is worth a thousand words,&quot; make sure it&#39;s conveying the intended message. Use this book to learn the principals of conveying precise meaning instead of serving as a distraction or being redundant. . Weapons of Math Destruction . It&#39;s good to be told/reminded that models are never neutral. . An Illustrated Book of Bad Arguments . Step one of avoiding cognitive biases is to know they exist meaning read this book! . Conclusion . Like I said, the number is small but the impact they&#39;ve had on me has been massive. Please comment below and tell me which books have impacted you the most during your journey. .",
            "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/b/2020/05/23/b-is-for-books.html",
            "relUrl": "/b/2020/05/23/b-is-for-books.html",
            "date": " • May 23, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "A is for Aim",
            "content": "TLDR: . This blog is NOT: . a collection of answers to Udemy’s Machine Learning Course | . This blog is: . a space for me to refine my thinking on machine learning principles and practices | an homage to An A-Z of ELT by Scott Thornbury | . Why another blog? . Good question. . True, I already have two blogs: one on EdTech and another on using R for data visualization. However, the real question is if a Google search for “data science blog” returns nearly half a BILLION results1, why should I create one? . Surely any opinion piece I write will have been written better by someone more qualified, right? . As for coding tutorials, again, won’t everything I create have already been done by someone with a PhD in computer science2? . While the answer to both of the questions above is more likely than not “yes”, it doesn’t detract from the actual aim of this enterprise which is to provide a space for me to solidify my understanding of data science/ machine learning. . How? . By writing about what I have learned, I will identity the areas where I am weak while solidifying the areas where I’m proficient. . But why do I think it will work? . Standing on the Shoulders of Giants . People who studied in the colleges of science and engineering are usually familiar Dr. Richard Feynmam. While he is famous for his work on quantum mechanics, his explanation of the Challenger disaster will forever be seared in my mind. . Why do I mention him? Feynman’s “notebook technique” is a model for learning anything: . Write down the concept you wish to master | Explain the concept in comprehensible language | Revise ideas/concepts which you can’t explain to a high standard | Identify complex areas and think of how to simplify them further (i.e., create analogies) | Or, to quote one of the true champions of teaching writing William Zinsser, “[w]riting and thinking and learning were the same process” and “Clear writing is the logical arrangement of thought; a scientist who thinks clearly can write as well as the best writer.”3 . Or, to put it yet another way, we can cite a quote attributed to John Dewey, “We do not learn from experience […] but from reflecting on an experience.”4 . Or, in my own words, I am writing this blog to identify and shift concepts, ideas, and practices from the “Don’t Know” column into the “Do Know”. . Consequently, nothing original will be found on this blog because my purpose isn’t to discover some novel statistical technique nor propose a new theory; my aim is far humbler: increase my understanding of machine learning by clarifying my thinking through the process of writing. Shouldn’t be that hard, right :smirk: . But why the name? . A little more than a decade ago when I was an English language teacher (ELT) in Hanoi, Vietnam, I was introduced to the work of Scott Thornbury, who is a legend in ELT circles, and whose blog An A-Z of ELT was required reading for my colleagues and I. . But why should I use a similar name? . First, structure: there are 26 letters in the English alphabet meaning, at the minimum, I’m compelled to write 26 entries. How long will that take? That’s a good question that I’m not even going to attempt to answer. . Second, I just like the name and, honestly, what more reason do I need? :laughing: . Footnotes . Google search May 14th, 2020 &#8617; . | That is an idea for a future project, “What is the ratio of data science coding tutorials written by holders of PhDs compared to non-PhDs?” &#8617; . | From the “Preface” to Writing to Learn &#8617; . | This quote is most likely an amalgamation of several ideas which Dewey professed, but I still like it :smiley: &#8617; . |",
            "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/a/2020/05/14/a-is-for-aim.html",
            "relUrl": "/a/2020/05/14/a-is-for-aim.html",
            "date": " • May 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I started coding in Python at the age of 35 while in graduate school (thanks Sowmya). Since then, I have learned the unbridled joy of using R for creating maps and attended a data science bootcamp in Berlin where I completed a projet using web scraping and machine learning (XGBoost, LSTM, and computer vision) to predict the minimum age necessary to read a book. . For a fuller picture of where I’ve been and what I’ve done, please see my portfolio . When I’m not working, I can be found doing Brazilian Jiu-Jitsu (BJJ), yoga, or reading sci-fi. .",
          "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "",
          "content": "User-agent: * Sitemap: https://educatorsrlearners.github.io/an-a-z-of-machine-learning/sitemap.xml .",
          "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

}