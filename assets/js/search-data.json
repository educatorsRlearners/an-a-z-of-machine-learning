{
  
    
        "post0": {
            "title": "D is for Decision Tree",
            "content": "What is a Decision Tree? . Decision Trees are an algorithm used for either classification or regression tasks. . How do they work? . While the math is fairly complicated, the concept is incredibly straightforward: . start with all observations in one group | identify a binary question, (i.e., yes/no, over/under) resulting in two groups which are as distinct from eachother as possible | repeat step two until every subgroup is homogeneous or some other metric has been achieved | What are the advantages? . Transparency: They are considered a white/glass box algorithm because you can see what decisions the algorithm made which leads to, . Interpretability: Again, since you can see which decisions were made, it&#39;s easy to comprehend and explain the predictions. . Ease: Data trees DO NOT require feature scaling or normalization1. . What are the disadvantages? . Overfitting: Since they try to find the purest groups, they have a tendency to overfit. . Non-Linear Data: Relationships in the data between features are not considered. . How do we train a decision tree? . Step 0: Frame the Problem . I&#39;m a task based person so let&#39;s set a problem. . Can we determine who would have survived the Titanic? . Step 1: Collect/Load our Data . Now let&#39;s get the data. . import seaborn as sns sns.set(palette=&quot;colorblind&quot;) titanic = sns.load_dataset(&quot;titanic&quot;) . Step 2: Inspect our Data . Do we have any missing values? . titanic.isnull().sum() . survived 0 pclass 0 sex 0 age 177 sibsp 0 parch 0 fare 0 embarked 2 class 0 who 0 adult_male 0 deck 688 embark_town 2 alive 0 alone 0 dtype: int64 . Yes we do. . What are the data types? . titanic.dtypes . survived int64 pclass int64 sex object age float64 sibsp int64 parch int64 fare float64 embarked object class category who object adult_male bool deck category embark_town object alive object alone bool dtype: object . We have a mix. . Let&#39;s get a feel for the values by looking at the first five rows. . titanic.head().T . 0 1 2 3 4 . survived 0 | 1 | 1 | 1 | 0 | . pclass 3 | 1 | 3 | 1 | 3 | . sex male | female | female | female | male | . age 22 | 38 | 26 | 35 | 35 | . sibsp 1 | 1 | 0 | 1 | 0 | . parch 0 | 0 | 0 | 0 | 0 | . fare 7.25 | 71.2833 | 7.925 | 53.1 | 8.05 | . embarked S | C | S | S | S | . class Third | First | Third | First | Third | . who man | woman | woman | woman | man | . adult_male True | False | False | False | True | . deck NaN | C | NaN | C | NaN | . embark_town Southampton | Cherbourg | Southampton | Southampton | Southampton | . alive no | yes | yes | yes | no | . alone False | False | True | False | True | . Hmmm. Looks like seaborn has already done some feature engineering (i.e, alone is a combination of sibsp and parch). Much appreciated :smiley: . Now, since this is a toy dataset, I&#39;m not going to do much EDA but, if you want to learn more, you can find some good examples of it here and here. . However, here is an obligatory bar chart of who survived. . sns.catplot(y=&quot;survived&quot;, hue=&quot;sex&quot;, kind=&quot;count&quot;, data=titanic); . Step 3: Prepare the data . In the interest of simplicity, I&#39;m going to make a REALLY basic model by: . dropping all features containing missing values | . titanic = titanic.dropna(axis=1) titanic.info(); . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 survived 891 non-null int64 1 pclass 891 non-null int64 2 sex 891 non-null object 3 sibsp 891 non-null int64 4 parch 891 non-null int64 5 fare 891 non-null float64 6 class 891 non-null category 7 who 891 non-null object 8 adult_male 891 non-null bool 9 alive 891 non-null object 10 alone 891 non-null bool dtypes: bool(2), category(1), float64(1), int64(4), object(3) memory usage: 58.5+ KB . keeping only numerical features | . titanic = titanic.select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]) titanic.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 survived 891 non-null int64 1 pclass 891 non-null int64 2 sibsp 891 non-null int64 3 parch 891 non-null int64 4 fare 891 non-null float64 dtypes: float64(1), int64(4) memory usage: 34.9 KB . Cool. . Time to make the train test split. . from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42) x = titanic.iloc[:, 1:] y = titanic.survived . and then . Step 4: Fit the model . from sklearn.tree import DecisionTreeClassifier tree_clf = DecisionTreeClassifier(random_state=0) tree_clf = tree_clf.fit(x_train, y_train) . Step 5: Evaluate the model . Visualzing the model . Earlier, I said it was easy to explain the decision the model made. . &quot;How?&quot; you ask? By visualizing it: . from sklearn.tree import export_graphviz from IPython.display import Image import pydotplus dot_data = export_graphviz(tree_clf, out_file=None, feature_names=x_train.columns, class_names=[&#39;perished&#39;, &#39;survived&#39;], rounded=True, filled=True) # Draw graph graph = pydotplus.graph_from_dot_data(dot_data) # Show graph Image(graph.create_png()) . That is a gigantic tree!!! In reality, we&#39;d probably prune it by limiting the depth of the tree in an attempt to avoid overfitting. . But, let see how it performs. . #Predict the response for test dataset y_pred = tree_clf.predict(x_test) from sklearn.metrics import accuracy_score # Model Accuracy, how often is the classifier correct? print(&quot;The training accuracy of our model is %.2f.&quot; % (accuracy_score(y_train, tree_clf.predict(x_train))*100)) print(&quot;The test accuracy of our model is %.2f.&quot; % (accuracy_score(y_test, y_pred) *100)) . The training accuracy of our model is 84.11. The test accuracy of our model is 69.40. . Yep, definitely overfitting. . So, let&#39;s prune our tree by limiting the max_depth to three: . tree_clf = DecisionTreeClassifier(max_depth=3, random_state=0) . retrain our model, . tree_clf = tree_clf.fit(x_train, y_train) . and visualize it one more time. . dot_data = export_graphviz(tree_clf, out_file=None, feature_names=x_train.columns, class_names=[&quot;perished&quot;, &quot;survived&quot;], rounded=True, filled=True) # Draw graph graph = pydotplus.graph_from_dot_data(dot_data) # Show graph Image(graph.create_png()) . That is a much easier tree to comprehend but does it perform better? . #Predict the response for test dataset y_pred = tree_clf.predict(x_test) # Model Accuracy, how often is the classifier correct? print(&quot;The training accuracy of our model is %.2f.&quot; % (accuracy_score(y_train, tree_clf.predict(x_train))*100)) print(&quot;The test accuracy of our model is %.2f.&quot; % (accuracy_score(y_test, y_pred) *100)) . The training accuracy of our model is 73.68. The test accuracy of our model is 71.64. . Indeed it does! . Now imagine how much better it would perform if we added gender and age to the model. . Summary . Decision trees are incredibly powerful because of their: . ease of use | flexibility | simplicity to understand | . However, overfitting can be a real issue so, as with all algorithms, check the training accuracy and tune the hyperparameters as needed. . Also, some of the splits defy common sense; for example, how can anyone have half a sibling or half a parent?2 . Therefore, while the trees maybe easy to interpret, they might not always be logical so, as always, we have to be prepared to defend what we&#39;ve made. . Happy coding! . Further Reading . Hands-On Machine Learning with Scikit-Learn and TensorFlow: &#39;Chapter 6 Decision Trees&#39; DataCamp Decision Tree Classifier Tutorial Understanding the Mathematics Behind Decision Trees Visualizing Decision Trees . Footnotes . 1. Geron, 2019, Page 177↩ . 2. If anyone knows how to set the tree to only split on whole values, please write it in the comments below.↩ .",
            "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/algorithm/2020/05/26/d-is-for-decision-tree.html",
            "relUrl": "/algorithm/2020/05/26/d-is-for-decision-tree.html",
            "date": " • May 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "C is for Classification",
            "content": "What is classification? . Classification is one of two types of supervised machine learning tasks with the other being regression. . Key point to remember: supervised learning tasks use features to predict targets, or, in non-tech speak, they use attributes to predict something. For instance, we can take a basketball players height, weight, age, foot-speed and/or multiple other aspects their them to predict how many points they&#39;ll score or whether they will be an all-star. . So what&#39;s the difference between the two? . Regression tasks predict a continuous value For example, how many points someone will score. | . | Classification tasks predict a non-continuous value. For example, whether a player will be an all-star. | . | . How do I know which technique to use? . Answer the following question: &quot;Does my target variable have an order to it?&quot; . For example, my project predicting the recommended age of a reader was a regression task because I was predicting a precise age (i.e., 4 years old). If I was attempting to identify whether a book was suitable for teens or not, then it would have been a classification task since the answer would have been either yes or no. . OK, so classification is only for yes/no, true/false, cat/dog problems, right? . Nope, those are just the easy examples :smiley: . Example 1: Sorting People into Groups . Imagine a scenario where you get a new batch of students every year and have to sort them into houses based on their personality characteristics. . . In this situation, the houses do not have any type of sequence/ranking to them. Sure, Harry definitely didn&#39;t want to be housed in Slytherin, and the sorting hat clearly took that into consideration, but that doesn&#39;t mean Slytherin is closer to Gryffindor like 25 is closer to 30 than it is to 19. . Example 2: Applying Labels . Similarly, if we had a data set containing the ingredients of dishes and attempted to predict the country of origin, we&#39;d be solving a classification problem. Why? Because country names have no numerical order. We can say that Russia is the largest country on Earth or that China has the most people but those are attributes of the country (i.e., land size and population) which are not intrinsic to the name of the country. . Got it! Numbers are regression, words are classification . Sorry but no. . Think back to the book recommendation project I mentioned earlier. Answer this: if I wanted to predict whether a book was for: . small children (2-5 years old) | primary children (6 - 10) | tweens (11 -12) | young adults (13 - 17) | adults (18 +) | . what would you use: regression or classification? . Well, given that the labels have a clear order, you&#39;d definitely want to treat this as a regression problem by coding &#39;small children&#39; as &#39;1&#39; and &#39;adults&#39; as &#39;5&#39;1. . In Sum . Always start by asking &#39;What am I attempting to predict?&#39; Once that question is solved the rest becomes far simpler because as Jung said, &quot;To ask the right question, is already half the problem solved.&quot; . Further Reading . Difference Between Classification and Regression in Machine Learning Regression Versus Classification Machine Learning: What’s the Difference? . Footnotes . 1. In R this type of variable is called an ordered factor↩ .",
            "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/concepts/2020/05/24/c-is-for-classification.html",
            "relUrl": "/concepts/2020/05/24/c-is-for-classification.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "B is for Books",
            "content": "While this is is not an exhaustive list of the books I&#39;ve read in my journey to become a data scientist, it is comprised of the books I consider my essential reading. . Statistics . The Cartoon Guide to Statistics . This is an engaging reference book which explains key concepts like descriptives statistics, distributions, probability, hypothesis testing. . The Manga Guide to Statistics . Similar to the title above, but this book has a plot that runs through it so, if like me, you find analogies easier to remember than formulas, this is a great book for you. . Data Science . R . R for Data Science . After completing several DataCamp courses, I started using R to analyze test-taker data and this book was my go to reference for the tidyverse. . Geocomputation in R . My last position required my team and I to travel throughout East Asia to deliver workshops, presentations, and provide client support and I used this book to teach myself how to make maps to visualize our reach and impact. . Blogdown: Creating Websites with R Markdown . I would not have been able to create my previous blog without this book. It is indispensable if you want to create a blog with RStudio. . Python . A Whirlwind Tour of Python . Need to get the gist of Python in a hurry? This is your book. . Hands-On Machine Learning with Scikit-Learn and TensorFlow . Chapter 2 End-to-End Machine Learning Project along with Appendix B: Machine Learning Project Checklist are worth the price of the book alone. . Python for Data Analysis . The real name of this book should be Pandas from A-Z. Seriously, work through the examples in Chapter 14 and you will feel infinitely more confident to use pandas in the future. . Concepts . Storytelling with Data . If &quot;a picture is worth a thousand words,&quot; we should make sure it&#39;s conveying our intended message. This book you make your visuals convey precise meaning at a glance instead of serving as a distraction or being redundant. . Weapons of Math Destruction . It&#39;s good to be reminded that models are never neutral. . An Illustrated Book of Bad Arguments . Usually, people don&#39;t want to think or act in a biased nature. However, if you&#39;re not aware that that a bias/fallacy exists, can you actively avoid it? Therefore, step one of avoiding cognitive biases is to know they exist meaning read this book! . Conclusion . Like I said, the number is small but the impact they&#39;ve had on me has been massive. Please comment below and tell me which books have impacted you the most during your journey. .",
            "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/resources/2020/05/23/b-is-for-books.html",
            "relUrl": "/resources/2020/05/23/b-is-for-books.html",
            "date": " • May 23, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "B is for Book",
            "content": "While this is not exhaustive list of the books I’ve read, it is what I consider my essential reading. Enjoy! . Statistics . The Cartoon Guide to Statistics . This is an engaging reference book which explains key concepts like descriptives statistics, distributions, probability, hypothesis testing. . The Manga Guide to Statistics . Similar to the title above, but this book has a plot that runs through it so, if like me, you find analogies easier to remember than formulas, this is a great book for you. . Data Science . R . R for Data Science . After completing several DataCamp courses, I started using R to analyze test-taker data and this book was my go to reference for the tidyverse. . Geocomputation in R . My last position required my team and I to travel throughout East Asia to deliver workshops, presentations, and provide client support and I used this book to teach myself how to make maps to visualize our reach and impact. . Blogdown: Creating Websites with R Markdown . I would not have been able to create my previous blog without this book. It is indispensable if you want to create a blog with RStudio. . Python . A Whirlwind Tour of Python . Need to get the gist of Python in a hurry? This is your book. . Hands-On Machine Learning with Scikit-Learn and TensorFlow . Chapter 2 End-to-End Machine Learning Project along with Appendix B: Machine Learning Project Checklist are worth the price of the book alone. . Python for Data Analysis . The real name of this book should be Pandas from A-Z. Seriously, work through the examples in Chapter 14 and you will feel infinitely more confident to use pandas in the future. . Concepts . Storytelling with Data . If “a picture is worth a thousand words,” we should make sure it’s conveying our intended message. This book you make your visuals convey precise meaning at a glance instead of serving as a distraction or being redundant. . Weapons of Math Destruction . It’s good to be reminded that models are never neutral. . An Illustrated Book of Bad Arguments . Usually, people don’t want to think or act in a biased nature. However, if you’re not aware that that a bias/fallacy exists, can you actively avoid it? Therefore, step one of avoiding cognitive biases is to know they exist meaning read this book! .",
            "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/opinion/2020/05/23/b-is-for-book.html",
            "relUrl": "/opinion/2020/05/23/b-is-for-book.html",
            "date": " • May 23, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "A is for Aim",
            "content": "TLDR: . This blog is NOT: . a collection of answers to Udemy’s Machine Learning Course | . This blog is: . a space for me to refine my thinking on machine learning principles and practices | an homage to An A-Z of ELT by Scott Thornbury | . Why another blog? . Good question. . True, I already have two blogs: one on EdTech and another on using R for data visualization. However, the real question is if a Google search for “data science blog” returns nearly half a BILLION results1, why should I create one? . Surely any opinion piece I write will have been written better by someone more qualified, right? . As for coding tutorials, again, won’t everything I create have already been done by someone with a PhD in computer science2? . While the answer to both of the questions above is more likely than not “yes”, it doesn’t detract from the actual aim of this enterprise which is to provide a space for me to solidify my understanding of data science/ machine learning. . How? . By writing about what I have learned, I will identity the areas where I am weak while solidifying the areas where I’m proficient. . But why do I think it will work? . Standing on the Shoulders of Giants . People who studied in the colleges of science and engineering are usually familiar Dr. Richard Feynmam. While he is famous for his work on quantum mechanics, his explanation of the Challenger disaster will forever be seared in my mind. . Why do I mention him? Feynman’s “notebook technique” is a model for learning anything: . Write down the concept you wish to master | Explain the concept in comprehensible language | Revise ideas/concepts which you can’t explain to a high standard | Identify complex areas and think of how to simplify them further (i.e., create analogies) | Or, to quote one of the true champions of teaching writing William Zinsser, “[w]riting and thinking and learning were the same process” and “Clear writing is the logical arrangement of thought; a scientist who thinks clearly can write as well as the best writer.”3 . Or, to put it yet another way, we can cite a quote attributed to John Dewey, “We do not learn from experience […] but from reflecting on an experience.”4 . Or, in my own words, I am writing this blog to identify and shift concepts, ideas, and practices from the “Don’t Know” column into the “Do Know”. . Consequently, nothing original will be found on this blog because my purpose isn’t to discover some novel statistical technique nor propose a new theory; my aim is far humbler: increase my understanding of machine learning by clarifying my thinking through the process of writing. Shouldn’t be that hard, right :smirk: . But why the name? . A little more than a decade ago when I was an English language teacher (ELT) in Hanoi, Vietnam, I was introduced to the work of Scott Thornbury, who is a legend in ELT circles, and whose blog An A-Z of ELT was required reading for my colleagues and I. . But why should I use a similar name? . First, structure: there are 26 letters in the English alphabet meaning, at the minimum, I’m compelled to write 26 entries. How long will that take? That’s a good question that I’m not even going to attempt to answer. . Second, I just like the name and, honestly, what more reason do I need? :laughing: . Footnotes . Google search May 14th, 2020 &#8617; . | That is an idea for a future project, “What is the ratio of data science coding tutorials written by holders of PhDs compared to non-PhDs?” &#8617; . | From the “Preface” to Writing to Learn &#8617; . | This quote is most likely an amalgamation of several ideas which Dewey professed, but I still like it :smiley: &#8617; . |",
            "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/opinion/2020/05/14/a-is-for-aim.html",
            "relUrl": "/opinion/2020/05/14/a-is-for-aim.html",
            "date": " • May 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I started coding in Python at the age of 35 while in graduate school (thanks Sowmya). Since then, I have learned the unbridled joy of using R for creating maps and attended a data science bootcamp in Berlin. . When I’m not working, I can be found on the mats doing Brazilian jiu-jitsu (BJJ), doing yoga, or reading sci-fi. .",
          "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://educatorsrlearners.github.io/an-a-z-of-machine-learning/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}